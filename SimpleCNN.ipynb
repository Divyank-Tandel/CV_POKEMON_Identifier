{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "475ec30f-71c1-4dfd-841a-90c406463c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "231b6be9-cf0a-4140-bb95-98c7930f297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a4e7324-da94-4b62-b08c-c22630c2a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 400, 400  # Use the pixel size of your images\n",
    "batch_size = 32\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c583d4a-a140-4553-bf9b-27461834e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75d5f317-2c26-4e51-b4f7-e6c678a204be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 276 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # For multi-class classification\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82841256-3865-4b24-ba20-266abdb34d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9de16ed9-454e-4235-a7d0-44712fe6de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Output layer with softmax for probabilities\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77f5e90a-8990-42f7-a349-9584004a6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46226e24-df26-4da8-9825-78ebf6e31923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/17 [==============================] - 49s 3s/step - loss: 4.2592 - accuracy: 0.6192 - val_loss: 0.4544 - val_accuracy: 0.7969\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 48s 3s/step - loss: 0.2833 - accuracy: 0.8692 - val_loss: 0.4178 - val_accuracy: 0.7969\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 49s 3s/step - loss: 0.1830 - accuracy: 0.9346 - val_loss: 0.2170 - val_accuracy: 0.9375\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 53s 3s/step - loss: 0.0892 - accuracy: 0.9731 - val_loss: 0.2544 - val_accuracy: 0.9531\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.0586 - accuracy: 0.9769 - val_loss: 0.5357 - val_accuracy: 0.8594\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0326 - accuracy: 0.9963 - val_loss: 0.4444 - val_accuracy: 0.8125\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 51s 3s/step - loss: 0.0232 - accuracy: 0.9926 - val_loss: 0.1393 - val_accuracy: 0.9688\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 52s 3s/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9206 - val_accuracy: 0.8750\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 54s 3s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1754 - val_accuracy: 0.9062\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 47s 3s/step - loss: 6.9090e-04 - accuracy: 1.0000 - val_loss: 0.4829 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "epochs = 10  # You can adjust the number of epochs\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff8c4378-efd0-48f3-b983-80f45f7469bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 482ms/step - loss: 0.4548 - accuracy: 0.8824\n",
      "Validation Accuracy: 0.8823529481887817\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ebeea04-1479-4f4b-8f85-78d1678d4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('character_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d502565-71ec-434d-a9b6-d806e9dca52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model_path, image_path, target_size):\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Normalize the image\n",
    "\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "     # Get the mapping from class indices to labels from the training generator\n",
    "    class_labels = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    print(class_labels)\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "    return predicted_class_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c6b2743-316d-40fb-b2f9-f84e591bd53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'character_recognition_model.h5'  # Path to your saved model\n",
    "single_image_path = 'test_images\\image.png'  # Path to the image you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "066d2695-db62-4f63-91c3-49820fd38c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 167ms/step\n",
      "{0: 'Minun_samePixel_data', 1: 'Plusle_samePixel_data', 2: 'pikachu_samePixel_data'}\n",
      "\n",
      "Predicted class for 'test_images\\image.png': pikachu_samePixel_data\n"
     ]
    }
   ],
   "source": [
    "predicted_class = predict_image(model_path, single_image_path, (img_height, img_width))\n",
    "print(f\"\\nPredicted class for '{single_image_path}': {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae4915-9d65-457e-aa89-8ce37df035a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836e9bb-79e7-4f67-912f-989a6a62893d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d770cb-9d92-4f7a-958a-3a248dcfd07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4b470-e9b6-4fc4-94d0-f3481981376a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f166434-a706-4a6a-a4d7-6746019bfd54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc002d-2a36-4968-b05a-cbfffe37ae05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c658e0-59c9-4445-9172-ce53cbeb4c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884338d-2638-4be2-9091-0ba473d805e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369bc10-f2ed-41ed-9831-4286f3397575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bec2da-b089-4333-85c1-d25fa7e592d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428f2df-556e-4113-9822-08e6073ba0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5efe2-df3f-455d-ba52-b2f3cfd25e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe7ed5-4541-4df4-976e-ce4199966cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d8243-6646-43a1-a277-9310d90865bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c3d2e-b8f2-4475-ab1b-6976b334aa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8864c27a-c891-427f-8084-3fd9d26c0c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9857611-c665-4377-9ee4-de61f8a333b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "61b679ed-8e0f-450a-a603-2ea5dcd7141b",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 1. Define the path to your image directories\n",
    "train_data_dir = 'path/to/your/main_directory'  # Contains subfolders: character_a, character_b, character_c\n",
    "\n",
    "# 2. Set image dimensions and batch size\n",
    "img_height, img_width = 128, 128  # Use the pixel size of your images\n",
    "batch_size = 32\n",
    "num_classes = 3  # Number of character classes\n",
    "\n",
    "# 3. Create an ImageDataGenerator for data loading and validation split\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2  # Split data into training and validation sets\n",
    ")\n",
    "\n",
    "# Load training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # For multi-class classification\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# 4. Define the CNN model architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')  # Output layer with softmax for probabilities\n",
    "])\n",
    "\n",
    "# 5. Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 6. Train the model\n",
    "epochs = 20  # You can adjust the number of epochs\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# 7. Evaluate the model (optional)\n",
    "# test_loss, test_accuracy = model.evaluate(validation_generator)\n",
    "# print(f\"Validation Accuracy: {test_accuracy}\")\n",
    "\n",
    "# 8. Save the trained model (optional)\n",
    "# model.save('character_recognition_model.h5')\n",
    "\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "def predict_image(model_path, image_path, target_size):\n",
    "    loaded_model = tf.keras.models.load_model(model_path)\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Normalize the image\n",
    "\n",
    "    predictions = loaded_model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "     # Get the mapping from class indices to labels from the training generator\n",
    "    class_labels = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    print(class_labels)\n",
    "    predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "    return predicted_class_label\n",
    "\n",
    "model_path = 'character_recognition_model.h5'  # Path to your saved model\n",
    "single_image_path = 'test_images\\image.png'  # Path to the image you want to predict\n",
    "\n",
    "predicted_class = predict_image(model_path, single_image_path, (img_height, img_width))\n",
    "print(f\"\\nPredicted class for '{single_image_path}': {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257bc488-7c6b-4302-8c60-5ddde03d4e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2262e0f-8f92-419e-a145-ff7344c73721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
